{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re as regex\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score,confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial.distance import cdist \n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading training and testing data\n",
    "training_data = pd.read_csv(r'C:\\Users\\Talha\\Desktop\\train.csv')\n",
    "testing_data = pd.read_csv(r'C:\\Users\\Talha\\Desktop\\test.csv')\n",
    "\n",
    "#reading stopwords\n",
    "my_file =open(\"C:\\\\Users\\\\Talha\\\\Desktop\\\\stop_words.txt\")\n",
    "content = my_file.read()\n",
    "stop_words = content.split(\"\\n\")\n",
    "my_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    \n",
    "    # Remove usernames\n",
    "    tweet = re.sub(r\"@[^\\s]+[\\s]?\",'',tweet)\n",
    "\n",
    "    \n",
    "    # remove URL\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "        \n",
    "    # remove special characters \n",
    "    tweet = re.sub('[^ a-zA-Z0-9]', '', tweet)\n",
    "    \n",
    "    # remove Numbers\n",
    "    tweet = re.sub('[0-9]', '', tweet)\n",
    "   \n",
    "    #changes to tweet to lowercase\n",
    "    tweet = tweet.lower()\n",
    "   \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying function clean_tweet to training data\n",
    "training_data['Tweet'] = training_data['Tweet'].apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>from amsterdam to ewr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>itproblems with the link thparty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>today the staff  msp took customer service to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>but have been yet to receive assistance from o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>you wont let me change my reservation online ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                              Tweet\n",
       "0   neutral                            from amsterdam to ewr  \n",
       "1  negative                   itproblems with the link thparty\n",
       "2  positive  today the staff  msp took customer service to ...\n",
       "3  negative  but have been yet to receive assistance from o...\n",
       "4  negative   you wont let me change my reservation online ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize the data\n",
    "def tweet_tokenize(tokenized_text):\n",
    "    return tokenized_text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['Tokenized Text']=training_data['Tweet'].apply(tweet_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Tokenized Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>from amsterdam to ewr</td>\n",
       "      <td>[from, amsterdam, to, ewr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>itproblems with the link thparty</td>\n",
       "      <td>[itproblems, with, the, link, thparty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>today the staff  msp took customer service to ...</td>\n",
       "      <td>[today, the, staff, msp, took, customer, servi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>but have been yet to receive assistance from o...</td>\n",
       "      <td>[but, have, been, yet, to, receive, assistance...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>you wont let me change my reservation online ...</td>\n",
       "      <td>[you, wont, let, me, change, my, reservation, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                              Tweet  \\\n",
       "0   neutral                            from amsterdam to ewr     \n",
       "1  negative                   itproblems with the link thparty   \n",
       "2  positive  today the staff  msp took customer service to ...   \n",
       "3  negative  but have been yet to receive assistance from o...   \n",
       "4  negative   you wont let me change my reservation online ...   \n",
       "\n",
       "                                      Tokenized Text  \n",
       "0                         [from, amsterdam, to, ewr]  \n",
       "1             [itproblems, with, the, link, thparty]  \n",
       "2  [today, the, staff, msp, took, customer, servi...  \n",
       "3  [but, have, been, yet, to, receive, assistance...  \n",
       "4  [you, wont, let, me, change, my, reservation, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter() \n",
    "for serial_no in training_data.index: #loop for all the values in the training data\n",
    "    words.update(training_data.loc[serial_no, \"Tokenized Text\"])#counts the number of words in the Tokenized Text column\n",
    "#words.most_common(11679)\n",
    "#print(serial_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the stopwords from text\n",
    "for serial_no, word in enumerate(stop_words):\n",
    "        del words[word]\n",
    "#print(serial_no)        \n",
    "#words.most_common(179)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes the list of words appearing in training data\n",
    "def dictionary(iterated_dataa):\n",
    "    \n",
    "    min_freq=1 \n",
    "    max_freq=3150 \n",
    "    dict_df = pd.DataFrame(data={\"Word\": [k for k, v in words.most_common() if min_freq < v < max_freq],\n",
    "                                 \"Frequency\": [v for k, v in words.most_common() if min_freq < v < max_freq]},\n",
    "                           columns=[\"Word\", \"Frequency\"]) #converts data into table format\n",
    "    #print(word_df)\n",
    "    vocab =dict_df.to_csv(r'C:\\Users\\Talha\\Desktop\\Dictionary.csv', index_label=\"serial no\")\n",
    "    dictionary = [k for k, v in words.most_common() if min_freq < v < max_freq]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary(training_data) #applies the dictionary function on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes bag of word\n",
    "dict_= []\n",
    "dict_df = pd.read_csv(r'C:\\Users\\Talha\\Desktop\\Dictionary.csv')\n",
    "dict_df = dict_df[dict_df[\"Frequency\"] > 1]\n",
    "dict_ = list(dict_df.loc[:, \"Word\"])\n",
    "\n",
    "label_column = [\"label\"]\n",
    "columns = label_column + list(map(lambda w: w + '',dict_))\n",
    "labels = []\n",
    "rows = []\n",
    "for serial_no in training_data.index:\n",
    "    current_row = []\n",
    "    \n",
    "    # add label to \n",
    "    current_label = training_data.loc[serial_no, \"Sentiment\"]\n",
    "    labels.append(current_label)\n",
    "    current_row.append(current_label)\n",
    "\n",
    "    # add bag-of-words\n",
    "    ref = set(training_data.loc[serial_no, \"Tokenized Text\"])\n",
    "    for _, word in enumerate(dict_):\n",
    "        current_row.append(1 if word in ref else 0) #makes the 0,1 vector\n",
    "\n",
    "    rows.append(current_row)\n",
    "\n",
    "data_vect = pd.DataFrame(rows, columns=columns) #makes the whole dataframe\n",
    "data_labels = pd.Series(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =data_vect.to_csv(r'C:\\Users\\Talha\\Desktop\\BOW.csv', index_label=\"serial no\") #creates the bow file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing of testing data\n",
    "def clean_testtweets(test_tweets):\n",
    "    \n",
    "    \n",
    "    # Remove usernames\n",
    "    test_tweet = re.sub(r\"@[^\\s]+[\\s]?\",'',test_tweet)\n",
    "    \n",
    "    \n",
    "    # remove URL\n",
    "    test_tweet = re.sub(r\"http\\S+\", \"\", test_tweet)\n",
    "    \n",
    "    # remove special characters \n",
    "    test_tweet = re.sub('[^ a-zA-Z0-9]', '', test_tweet)\n",
    "    \n",
    "    # remove Numbers\n",
    "    test_tweet = re.sub('[0-9]', '', test_tweet)\n",
    "    test_tweet = test_tweet.lower()\n",
    "    \n",
    "    return test_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data['Tweet'] = testing_data['Tweet'].apply(clean_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data['Tokenized Text']=testing_data['Tweet'].apply(tweet_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "for serial_no in testing_data.index:\n",
    "    words.update(testing_data.loc[serial_no, \"Tokenized Text\"])\n",
    "#words.most_common(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serial_no, word in enumerate(stop_words):\n",
    "        del words[word]\n",
    "#print(serial_no)        \n",
    "#words.most_common(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes bag of word\n",
    "dict_= []\n",
    "dict_df = pd.read_csv(r'C:\\Users\\Talha\\Desktop\\Dictionary.csv')\n",
    "dict_df = dict_df[dict_df[\"Frequency\"] > 1]\n",
    "dict_ = list(dict_df.loc[:, \"Word\"])\n",
    "\n",
    "label_column = [\"label\"]\n",
    "columns = label_column + list(map(lambda w: w + '',dict_))\n",
    "labels = []\n",
    "rows = []\n",
    "for serial_no in testing_data.index:\n",
    "    current_row = []\n",
    "    \n",
    "    # add label to \n",
    "    current_label = testing_data.loc[serial_no, \"Sentiment\"]\n",
    "    labels.append(current_label)\n",
    "    current_row.append(current_label)\n",
    "\n",
    "    # add bag-of-words\n",
    "    ref = set(testing_data.loc[serial_no, \"Tokenized Text\"])\n",
    "    for _, word in enumerate(dict_):\n",
    "        current_row.append(1 if word in ref else 0) #makes the 0,1 vector\n",
    "\n",
    "    rows.append(current_row)\n",
    "\n",
    "test_data_vect = pd.DataFrame(rows, columns=columns) #makes the whole dataframe\n",
    "data_labels = pd.Series(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file =test_data_vect.to_csv(r'C:\\Users\\Talha\\Desktop\\BOW_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_vect = pd.read_csv(r'C:\\Users\\Talha\\Desktop\\BOW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_vect = pd.read_csv(r'C:\\Users\\Talha\\Desktop\\BOW_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the data into numpy arrays\n",
    "#Also seperated the labels from features\n",
    "X_train = train_data_vect.to_numpy()\n",
    "X_train = X_train[:,2:2912]\n",
    "\n",
    "Y_train = train_data_vect.to_numpy()\n",
    "Y_train = Y_train[:,1]\n",
    "\n",
    "X_test = test_data_vect.to_numpy()\n",
    "X_test = X_test[:,2:2912]\n",
    "\n",
    "Y_test = test_data_vect.to_numpy()\n",
    "Y_test = Y_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Calculate Euclidean Distances between testing set and training set\n",
    "    distance = cdist(X_test, X_train, 'euclidean')\n",
    "    #Sorts array indices\n",
    "    sorted_distance = np.argsort(distance)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    K= [1, 3, 5, 7, 10]\n",
    "    accuracy=[]\n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    F1=[]\n",
    "    #predicts label\n",
    "    predicted_label= []\n",
    "    for i in range(len(sorted_distance)):\n",
    "            freq = []\n",
    "            for j in range(K):\n",
    "                freq.append(X_test.item(sorted_distance.item((i,j))))\n",
    "                predicted_label.append(collections.Counter(freq).most_common(1)[0][0])\n",
    "    #finds macrovaerages and plots             \n",
    "    for k in K:\n",
    "        \n",
    "        print('k = ', k)\n",
    "        \n",
    "        precision_p =0\n",
    "        precision_neg =0\n",
    "        precision_neu = 0\n",
    "        precision_ = 0\n",
    "        \n",
    "        recall_p = 0\n",
    "        recall_neg = 0               \n",
    "        recall_neu = 0\n",
    "        recall_ = 0\n",
    "        \n",
    "        F1_=0\n",
    "        \n",
    "        accuracy_ = 0 \n",
    "        \n",
    "\n",
    "        for i in range(len(predicted_label)):\n",
    "            \n",
    "            df_confusion_matrix = pd.crosstab(Y_test[i],predicted_label[i])\n",
    "            arr = np.df_confusion_matrix\n",
    "                       \n",
    "            \n",
    "            precision_p = (arr[0,0])/(arr[0,0]+arr[0,1]+ar[0,2])\n",
    "            precision_neg = (arr[1,1])/(arr[1,1]+arr[1,1]+arr[1,2])\n",
    "            precision_neu = (arr[2,2])/(arr[2,2]+arr[2,1],arr[2,0])\n",
    "            precision_ = (precision_p+precision_neg+precision_neu)/3\n",
    "            print('Macroaverage Precision: ',macroaverage_precision)\n",
    "\n",
    "            recall_p = (arr[0,0])/(arr[1,0]+arr[2,1]+arr[0,0])\n",
    "            recall_neg = (arr[1,1])/(arr[1,1]+arr[2,1]+arr[0,1])               \n",
    "            recall_neu = (arr[2,2])/(arr[2,2]+arr[1,2]+arr[0,2])\n",
    "            recall_ = (recall_p+recall_neg+recall_neu)/3\n",
    "            print('Macroaverage recall: ',macroaverage_recall)\n",
    "\n",
    "            F1_=2/(1/macroaverage_precision+1/macroaverage_recall)\n",
    "            print('F1: ',f1)\n",
    "\n",
    "            accuracy = (arr[0,0]+arr[1,1]+arr[2,2])/(arr[0,0]+arr[1,1]+arr[2,2]+arr[0,1]+arr[1,0]+arr[1,2]+arr[2,1]) \n",
    "                       \n",
    "            accuracy.append(accuracy_)\n",
    "            precision.append(precision_)\n",
    "            recall.append(recall_)\n",
    "            F1.append(F1_)\n",
    "\n",
    "    plt.plot(k,accuracy_)\n",
    "    plt.plot(k,recall_)\n",
    "    plt.plot(k,precision_)\n",
    "    plt.plot(k,F1_)\n",
    "    plt.legend(('Accuracy','Recall','Precision','F1'))\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Results')\n",
    "    \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    words = [word for word in data.split() if word in word2vec.vocab]\n",
    "    result = np.mean(word2vec[words], axis=0)\n",
    "    return result\n",
    "\n",
    "training_word2vec = X_train.apply(lambda x: extract_features(x))\n",
    "testing_word2vec = X_test.apply(lambda x: extract_features(x))\n",
    "\n",
    "\n",
    "print('\\nPart 3 for Part 1:\\n')\n",
    "    #Calculate Euclidean Distances between testing set and training set\n",
    "    distance = cdist(testing_word2vec, training_word2vec, 'euclidean')\n",
    "    #Sorts array indices\n",
    "    sorted_distance = np.argsort(distance)\n",
    "    \n",
    "\n",
    "    K= [1, 3, 5, 7, 10]\n",
    "    accuracy=[]\n",
    "    precision=[]\n",
    "    recall=[]\n",
    "    F1=[]\n",
    "    #predicts label\n",
    "    predicted_label= []\n",
    "        for i in range(len(sorted_distance)):\n",
    "            freq = []\n",
    "            for j in range(K):\n",
    "                freq.append(labels.item(sorted_distance.item((i,j))))\n",
    "                predicted_label.append(collections.Counter(votes).most_common(1)[0][0])\n",
    "    #finds macrovaerages and plots             \n",
    "    for k in K:\n",
    "        \n",
    "        print('k = ', k)\n",
    "        \n",
    "        precision_p =0\n",
    "        precision_neg =0\n",
    "        precision_neu = 0\n",
    "        precision_ = 0\n",
    "        \n",
    "        recall_p = 0\n",
    "        recall_neg = 0               \n",
    "        recall_neu = 0\n",
    "        recall_ = 0\n",
    "        \n",
    "        F1_=0\n",
    "        \n",
    "        accuracy_ = 0 \n",
    "        \n",
    "\n",
    "        for i in range(len(predicted_label):\n",
    "            \n",
    "            df_confusion_matrix = pd.crosstab(testing_word2vec[i],predicted_label[i])\n",
    "            arr = np.df_confusion_matrix\n",
    "                       \n",
    "            \n",
    "            precision_p = (arr[0,0])/(arr[0,0]+arr[0,1]+ar[0,2])\n",
    "            precision_neg = (arr[1,1])/(arr[1,1]+arr[1,1]+arr[1,2])\n",
    "            precision_neu = (arr[2,2])/(arr[2,2]+arr[2,1],arr[2,0])\n",
    "            precision_ = (precision_p+precision_neg+precision_neu)/3\n",
    "            print('Macroaverage Precision: ',macroaverage_precision)\n",
    "\n",
    "            recall_p = (arr[0,0])/(arr[1,0]+arr[2,1]+arr[0,0])\n",
    "            recall_neg = (arr[1,1])/(arr[1,1]+arr[2,1]+arr[0,1])               \n",
    "            recall_neu = (arr[2,2])/(arr[2,2]+arr[1,2]+arr[0,2])\n",
    "            recall_ = (recall_p+recall_neg+recall_neu)/3\n",
    "            print('Macroaverage recall: ',macroaverage_recall)\n",
    "\n",
    "            F1_=2/(1/macroaverage_precision+1/macroaverage_recall)\n",
    "            print('F1: ',f1)\n",
    "\n",
    "            accuracy = (arr[0,0]+arr[1,1]+arr[2,2])/(arr[0,0]+arr[1,1]+arr[2,2]+arr[0,1]+arr[1,0]+arr[1,2]+arr[2,1]) \n",
    "                       \n",
    "            accuracy.append(accuracy_)\n",
    "            precision.append(precision_)\n",
    "            recall.append(recall_)\n",
    "            F1.append(F1_)\n",
    "\n",
    "    plt.plot(k,accuracy)\n",
    "    plt.plot(k,recall)\n",
    "    plt.plot(k,precision)\n",
    "    plt.plot(k,F1)\n",
    "    plt.legend(('Accuracy','Recall','Precision','F1'))\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Results')\n",
    "    \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
